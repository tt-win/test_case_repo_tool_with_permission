#!/usr/bin/env python3
"""
è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ï¼ˆä¸ä¾è³´ migrate.pyï¼‰

åŠŸèƒ½ï¼š
- ä»¥ app.models.database_models.Base ç‚ºå”¯ä¸€çœŸå¯¦ä¾†æºå»ºç«‹è³‡æ–™è¡¨
- æª¢æŸ¥ã€Œé‡è¦è¡¨ã€æ˜¯å¦å­˜åœ¨
- æƒæã€Œé—œéµæ¬„ä½ã€ç¼ºå¤±ï¼ˆé è¨­åƒ…å ±å‘Šï¼‰ï¼Œå¯é¸æ“‡å®‰å…¨è‡ªå‹•æ–°å¢ï¼ˆ--auto-fixï¼‰
- ç¢ºä¿å¸¸ç”¨ç´¢å¼•å­˜åœ¨
- SQLite è‡ªå‹•å‚™ä»½ï¼ˆå¯ç”¨ --no-backup é—œé–‰ï¼‰
- æä¾›çµ±è¨ˆè¼¸å‡ºï¼ˆ--stats-only åƒ…è¼¸å‡ºçµ±è¨ˆä¸è®Šæ›´è³‡æ–™åº«ï¼‰

ä½¿ç”¨ï¼š
  python database_init.py [--auto-fix] [--no-backup] [--stats-only] [--verbose | --quiet]

æ³¨æ„ï¼š
- åƒ…æ–°å¢æ¬„ä½ï¼Œä¸åšç ´å£æ€§è®Šæ›´ï¼ˆä¸æ”¹å‹ã€ä¸åˆªæ¬„ã€ä¸æ”¹éµ/ç´„æŸï¼‰
- åš´ç¦æ··ç”¨ä¸åŒ Baseï¼›æœ¬è…³æœ¬å›ºå®šæ¡ç”¨ app.models.database_models.Base
"""

from __future__ import annotations

import os
import sys
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

# ç¢ºä¿å°ˆæ¡ˆæ ¹ç›®éŒ„åœ¨åŒ¯å…¥è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from sqlalchemy import text
from sqlalchemy import inspect
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError

from app.database import engine  # åƒ…åŒ¯å…¥ engineï¼Œä¿ç•™è©²æª”çš„ SQLite å„ªåŒ–è¨­å®š
from app.models.database_models import (
    Base,
    Team, TestRunConfig, TestRunItem, TestRunItemResultHistory,
    TCGRecord, LarkDepartment, LarkUser, SyncHistory,
)

# -----------------------------
# è¼”åŠ©è¼¸å‡ºï¼ˆç¹é«”ä¸­æ–‡ï¼‰
# -----------------------------
class Logger:
    def __init__(self, verbose: bool = False, quiet: bool = False):
        self.verbose = verbose
        self.quiet = quiet

    def info(self, msg: str):
        if not self.quiet:
            print(f"[INFO] {msg}")

    def debug(self, msg: str):
        if self.verbose and not self.quiet:
            print(f"[VERBOSE] {msg}")

    def warn(self, msg: str):
        print(f"[WARN] {msg}")

    def error(self, msg: str):
        print(f"[ERROR] {msg}")


# -----------------------------
# é€šç”¨å·¥å…·
# -----------------------------
IMPORTANT_TABLES: List[str] = [
    "teams",
    "test_run_configs",
    "test_run_items",
    "test_run_item_result_history",
    "tcg_records",
    "lark_departments",
    "lark_users",
    "sync_history",
]


def is_sqlite(engine: Engine) -> bool:
    return (engine.dialect.name or "").lower() == "sqlite"


def quote_ident(engine: Engine, name: str) -> str:
    return engine.dialect.identifier_preparer.quote(name)


# æ¬„ä½è¦æ ¼
class ColumnSpec:
    def __init__(self, name: str, type_sql: str, nullable: bool = True,
                 default: Optional[Any] = None, notes: Optional[str] = None):
        self.name = name
        self.type_sql = type_sql
        self.nullable = nullable
        self.default = default
        self.notes = notes

    def safe_to_add_on(self, engine: Engine) -> bool:
        # å®‰å…¨æ–°å¢è¦å‰‡ï¼š
        # - å¯ç‚º NULL çš„æ¬„ä½
        # - æˆ– NOT NULL ä½†æä¾› DEFAULT
        if self.nullable:
            return True
        return self.default is not None

    def default_sql_literal(self) -> Optional[str]:
        if self.default is None:
            return None
        if isinstance(self.default, str):
            return "'" + self.default.replace("'", "''") + "'"
        if self.default is True:
            return "1"
        if self.default is False:
            return "0"
        if self.default is None:
            return "NULL"
        return str(self.default)


# æ¬„ä½æª¢æŸ¥æ¸…å–®ï¼ˆåƒ…åˆ—å‡ºå¯èƒ½åœ¨æ—¢æœ‰ DB ç¼ºå°‘ã€ä¸”å¯ç”±æˆ‘å€‘è¼•é‡è£œä¸Šçš„æ¬„ä½ï¼‰
COLUMN_CHECKS: Dict[str, List[ColumnSpec]] = {
    # TestRunItem çµæœæª”æ¡ˆè¿½è¹¤æ¬„ä½
    "test_run_items": [
        ColumnSpec("result_files_uploaded", "INTEGER", nullable=False, default=0),
        ColumnSpec("result_files_count", "INTEGER", nullable=False, default=0),
        ColumnSpec("upload_history_json", "TEXT", nullable=True, default=None),
        # èˆŠæ¬„ä½æª¢æŸ¥ï¼ˆå­˜åœ¨å³å¯ï¼Œä¸æœƒè‡ªå‹•å»ºç«‹ NOT NULL ç„¡é è¨­çš„æ¬„ä½ï¼‰
        ColumnSpec("assignee_json", "TEXT", nullable=True, default=None),
        ColumnSpec("tcg_json", "TEXT", nullable=True, default=None),
        ColumnSpec("bug_tickets_json", "TEXT", nullable=True, default=None),
    ],
    # TestRunConfig çš„ TP ç¥¨æ¬„ä½èˆ‡é€šçŸ¥æ¬„ä½
    "test_run_configs": [
        # TP ç¥¨ç›¸é—œ
        ColumnSpec("related_tp_tickets_json", "TEXT", nullable=True, default=None),
        ColumnSpec("tp_tickets_search", "TEXT", nullable=True, default=None),
        # é€šçŸ¥ç›¸é—œï¼ˆå°æ‡‰ ORMï¼šnotifications_enabled, notify_chat_ids_json, notify_chat_names_snapshot, notify_chats_searchï¼‰
        ColumnSpec("notifications_enabled", "INTEGER", nullable=False, default=0),  # Boolean -> INTEGER(0/1)
        ColumnSpec("notify_chat_ids_json", "TEXT", nullable=True, default=None),
        ColumnSpec("notify_chat_names_snapshot", "TEXT", nullable=True, default=None),
        ColumnSpec("notify_chats_search", "TEXT", nullable=True, default=None),
    ],
    # Lark Users é‡è¦ç´¢å¼•æ¬„ä½ï¼ˆè‹¥ç¼ºå°‘æ¬„ä½å‰‡åƒ…å ±å‘Šï¼Œä¸å¼·åˆ¶æ–°å¢ NOT NULLï¼‰
    "lark_users": [
        ColumnSpec("enterprise_email", "TEXT", nullable=True, default=None),
        ColumnSpec("primary_department_id", "TEXT", nullable=True, default=None),
    ],
}

# ç´¢å¼•è¦æ ¼
INDEX_SPECS: List[Dict[str, Any]] = [
    {"name": "idx_tri_configid_testcaseno", "table": "test_run_items", "columns": ["config_id", "test_case_number"]},
    {"name": "idx_tri_teamid_result", "table": "test_run_items", "columns": ["team_id", "test_result"]},
    {"name": "idx_tri_result_files_uploaded", "table": "test_run_items", "columns": ["result_files_uploaded"]},
    # test_run_configs ç›¸é—œæœå°‹æ¬„ä½ç´¢å¼•ï¼ˆè‹¥ ORM å·²å»ºç«‹ï¼Œé€™è£¡ä»¥ IF NOT EXISTS å½¢å¼è£œå¼·ï¼‰
    {"name": "idx_trc_tp_tickets_search", "table": "test_run_configs", "columns": ["tp_tickets_search"]},
    {"name": "idx_trc_notify_chats_search", "table": "test_run_configs", "columns": ["notify_chats_search"]},
    # Lark Users å¸¸ç”¨ç´¢å¼•
    {"name": "idx_lu_enterprise_email", "table": "lark_users", "columns": ["enterprise_email"]},
    {"name": "idx_lu_primary_department_id", "table": "lark_users", "columns": ["primary_department_id"]},
    # Sync History
    {"name": "idx_sh_teamid_starttime", "table": "sync_history", "columns": ["team_id", "start_time"]},
]


# -----------------------------
# æ ¸å¿ƒæ­¥é©Ÿå¯¦ä½œ
# -----------------------------

def backup_sqlite_if_needed(engine: Engine, logger: Logger) -> Optional[str]:
    if not is_sqlite(engine):
        logger.debug("é SQLiteï¼Œç•¥éå‚™ä»½ç¨‹åº")
        return None
    db_path = engine.url.database
    if not db_path or db_path == ":memory:":
        logger.debug("SQLite è¨˜æ†¶é«”è³‡æ–™åº«ï¼Œç•¥éå‚™ä»½")
        return None
    if not os.path.exists(db_path):
        logger.debug(f"è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨ï¼ˆå°‡æ–¼ create_all æ™‚å»ºç«‹ï¼‰ï¼š{db_path}")
        return None
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"backup_init_{ts}.db"
    try:
        shutil.copy2(db_path, backup_path)
        logger.info(f"å·²å»ºç«‹ SQLite å‚™ä»½ï¼š{backup_path}")
        return backup_path
    except Exception as e:
        logger.warn(f"å»ºç«‹å‚™ä»½å¤±æ•—ï¼ˆä¸ä¸­æ–·ï¼‰ï¼š{e}")
        return None


def create_all_tables(engine: Engine, logger: Logger):
    logger.info("å»ºç«‹/ç¢ºä¿æ‰€æœ‰è³‡æ–™è¡¨ï¼ˆä¾æ“š ORM æ¨¡å‹ï¼‰...")
    try:
        Base.metadata.create_all(bind=engine)
        logger.info("è³‡æ–™è¡¨ç¢ºèªå®Œæˆ")
    except SQLAlchemyError as e:
        raise RuntimeError(f"å»ºç«‹è³‡æ–™è¡¨å¤±æ•—ï¼š{e}")


def verify_required_tables(engine: Engine, logger: Logger) -> Tuple[bool, List[str]]:
    inspector = inspect(engine)
    existing = {t.lower() for t in inspector.get_table_names()}
    missing = [t for t in IMPORTANT_TABLES if t.lower() not in existing]
    if missing:
        logger.error(f"ç¼ºå°‘é‡è¦è¡¨ï¼š{missing}")
        return False, missing
    logger.debug("æ‰€æœ‰é‡è¦è¡¨çš†å­˜åœ¨")
    return True, []


def get_existing_columns(engine: Engine, table_name: str) -> Dict[str, Dict[str, Any]]:
    # ä»¥å°å¯« key å›å‚³
    result: Dict[str, Dict[str, Any]] = {}
    if is_sqlite(engine):
        with engine.connect() as conn:
            rows = conn.exec_driver_sql(f"PRAGMA table_info({table_name})").fetchall()
            # PRAGMA columns: cid, name, type, notnull, dflt_value, pk
            for _, name, typ, notnull, dflt, _ in rows:
                result[(name or "").lower()] = {
                    "name": name,
                    "type": typ,
                    "notnull": bool(notnull),
                    "default": dflt,
                }
    else:
        inspector = inspect(engine)
        cols = inspector.get_columns(table_name)
        for col in cols:
            result[(col.get("name") or "").lower()] = col
    return result


def check_missing_columns(engine: Engine, logger: Logger) -> Dict[str, List[ColumnSpec]]:
    missing: Dict[str, List[ColumnSpec]] = {}
    for table, specs in COLUMN_CHECKS.items():
        try:
            existing = get_existing_columns(engine, table)
        except Exception:
            # è¡¨ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œäº¤ç”± verify_required_tables å…ˆè¡Œè™•ç†
            continue
        for spec in specs:
            if spec.name.lower() not in existing:
                missing.setdefault(table, []).append(spec)
    if missing:
        logger.warn("åµæ¸¬åˆ°ç¼ºå¤±æ¬„ä½ï¼ˆé è¨­åƒ…å ±å‘Šï¼Œä¸è‡ªå‹•ä¿®å¾©ï¼‰ï¼š")
        for table, specs in missing.items():
            for spec in specs:
                fixable = "å¯å®‰å…¨æ–°å¢" if spec.safe_to_add_on(engine) else "éœ€äººå·¥è™•ç†"
                logger.warn(f"  - {table}.{spec.name} ({spec.type_sql}) -> {fixable}{'ï½œ' + spec.notes if spec.notes else ''}")
    else:
        logger.info("æœªç™¼ç¾éœ€è£œå……çš„æ¬„ä½")
    return missing


def auto_fix_columns(engine: Engine, logger: Logger, missing: Dict[str, List[ColumnSpec]]):
    if not missing:
        logger.info("ç„¡æ¬„ä½éœ€è¦è‡ªå‹•ä¿®å¾©")
        return
    logger.info("é–‹å§‹è‡ªå‹•æ–°å¢å®‰å…¨æ¬„ä½ï¼ˆåƒ…é™å¯å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼‰...")
    for table, specs in missing.items():
        for spec in specs:
            if not spec.safe_to_add_on(engine):
                logger.warn(f"è·³éä¸å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼š{table}.{spec.name}ï¼ˆNOT NULL ä¸”ç„¡ DEFAULT æˆ–éœ€äººå·¥é·ç§»ï¼‰")
                continue
            parts = [spec.type_sql]
            default_sql = spec.default_sql_literal()
            if default_sql is not None:
                parts.append(f"DEFAULT {default_sql}")
            if not spec.nullable:
                parts.append("NOT NULL")
            col_ddl = " ".join(parts)
            sql = f"ALTER TABLE {quote_ident(engine, table)} ADD COLUMN {quote_ident(engine, spec.name)} {col_ddl}"
            try:
                with engine.begin() as conn:
                    conn.exec_driver_sql(sql)
                logger.info(f"å·²æ–°å¢æ¬„ä½ï¼š{table}.{spec.name}")
            except Exception as e:
                logger.warn(f"æ–°å¢æ¬„ä½å¤±æ•—ï¼š{table}.{spec.name} -> {e}")


def ensure_indexes(engine: Engine, logger: Logger):
    logger.info("ç¢ºä¿å¸¸ç”¨ç´¢å¼•å­˜åœ¨...")
    dialect = (engine.dialect.name or "").lower()
    supports_if_not_exists = dialect in {"sqlite", "postgresql"}
    inspector = inspect(engine)

    for idx in INDEX_SPECS:
        name = idx["name"]
        table = idx["table"]
        columns = idx["columns"]
        try:
            existing = {i.get("name") for i in inspector.get_indexes(table)}
        except Exception:
            existing = set()
        if name in existing:
            logger.debug(f"ç´¢å¼•å·²å­˜åœ¨ï¼š{name}")
            continue
        cols_sql = ", ".join(quote_ident(engine, c) for c in columns)
        if supports_if_not_exists:
            sql = f"CREATE INDEX IF NOT EXISTS {quote_ident(engine, name)} ON {quote_ident(engine, table)} ({cols_sql})"
        else:
            sql = f"CREATE INDEX {quote_ident(engine, name)} ON {quote_ident(engine, table)} ({cols_sql})"
        try:
            with engine.begin() as conn:
                conn.exec_driver_sql(sql)
            logger.info(f"å·²å»ºç«‹ç´¢å¼•ï¼š{name}")
        except Exception as e:
            # å¯èƒ½ç«¶æ…‹æˆ–å·²å­˜åœ¨ç­‰æƒ…æ³
            logger.warn(f"å»ºç«‹ç´¢å¼•è­¦å‘Šï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰ï¼š{name} -> {e}")


def get_database_stats(engine: Engine, logger: Logger) -> Dict[str, Any]:
    stats: Dict[str, Any] = {"tables": {}, "total_tables": 0, "engine_url": str(engine.url), "errors": []}
    try:
        if is_sqlite(engine):
            with engine.connect() as conn:
                rows = conn.exec_driver_sql(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
                ).fetchall()
                table_names = [r[0] for r in rows]
                for t in table_names:
                    try:
                        cnt = conn.exec_driver_sql(f"SELECT COUNT(*) FROM {quote_ident(engine, t)}").scalar()
                        cols = conn.exec_driver_sql(f"PRAGMA table_info({t})").fetchall()
                        stats["tables"][t] = {
                            "rows": int(cnt or 0),
                            "columns": len(cols),
                        }
                    except Exception as e:
                        stats["tables"][t] = {"error": str(e)}
        else:
            inspector = inspect(engine)
            table_names = inspector.get_table_names()
            with engine.connect() as conn:
                for t in table_names:
                    try:
                        cnt = conn.execute(text(f"SELECT COUNT(*) FROM {quote_ident(engine, t)}")).scalar()
                        cols = inspector.get_columns(t)
                        stats["tables"][t] = {
                            "rows": int(cnt or 0),
                            "columns": len(cols),
                        }
                    except Exception as e:
                        stats["tables"][t] = {"error": str(e)}
        stats["total_tables"] = len(stats["tables"])
    except Exception as e:
        stats["errors"].append(str(e))
    return stats


def print_stats(stats: Dict[str, Any], logger: Logger):
    print("=" * 60)
    print("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆæ‘˜è¦")
    print("=" * 60)
    print(f"ç¸½è¡¨æ ¼æ•¸ï¼š{stats.get('total_tables')}")
    tables = stats.get("tables", {})
    for t, d in sorted(tables.items()):
        if "error" in d:
            print(f"  âŒ {t}: {d['error']}")
        else:
            print(f"  âœ… {t}: {d['rows']} ç­†è¨˜éŒ„, {d['columns']} æ¬„ä½")
    print()
    print("é‡è¦è¡¨æ ¼ç‹€æ…‹ï¼š")
    for t in IMPORTANT_TABLES:
        d = tables.get(t)
        if d is None:
            print(f"  âš ï¸  {t}: è¡¨æ ¼ä¸å­˜åœ¨")
        elif "error" in d:
            print(f"  âŒ {t}: {d['error']}")
        else:
            print(f"  âœ… {t}: {d['rows']} ç­†è¨˜éŒ„, {d['columns']} æ¬„ä½")
    print()
    print(f"ğŸ“‚ è³‡æ–™åº«ä½ç½®ï¼š{stats.get('engine_url')}")


# -----------------------------
# åƒæ•¸èˆ‡ä¸»æµç¨‹
# -----------------------------

def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ï¼ˆä¸ä¾è³´ migrate.pyï¼‰")
    p.add_argument("--auto-fix", action="store_true", help="è‡ªå‹•æ–°å¢å¯å®‰å…¨æ–°å¢çš„ç¼ºå¤±æ¬„ä½")
    p.add_argument("--no-backup", action="store_true", help="ï¼ˆSQLiteï¼‰è·³éåˆå§‹åŒ–å‰çš„è³‡æ–™åº«å‚™ä»½")
    p.add_argument("--stats-only", action="store_true", help="åƒ…è¼¸å‡ºçµ±è¨ˆèˆ‡ç‹€æ…‹ï¼Œä¸åšä»»ä½•è®Šæ›´")
    g = p.add_mutually_exclusive_group()
    g.add_argument("--verbose", action="store_true", help="è¼¸å‡ºæ›´å¤šè©³ç´°è³‡è¨Š")
    g.add_argument("--quiet", action="store_true", help="åƒ…è¼¸å‡ºå¿…è¦è³‡è¨Šèˆ‡éŒ¯èª¤")
    return p.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv)
    logger = Logger(verbose=args.verbose, quiet=args.quiet)

    print("=" * 60)
    print("ğŸ—ƒï¸  è³‡æ–™åº«åˆå§‹åŒ–ç³»çµ±ï¼ˆä¸ä¾è³´ migrate.pyï¼‰")
    print("=" * 60)

    try:
        db_url = str(engine.url)
        db_kind = engine.dialect.name
        logger.info(f"åµæ¸¬åˆ°è³‡æ–™åº«ï¼š{db_kind} | URL={db_url}")

        if args.stats_only:
            stats = get_database_stats(engine, logger)
            print_stats(stats, logger)
            return 0

        # å‚™ä»½ï¼ˆSQLiteï¼‰
        backup_path = None
        if is_sqlite(engine) and not args.no_backup:
            backup_path = backup_sqlite_if_needed(engine, logger)

        # å»ºè¡¨
        create_all_tables(engine, logger)

        # é©—è­‰é‡è¦è¡¨
        ok, missing = verify_required_tables(engine, logger)
        if not ok:
            logger.error("é‡è¦è¡¨ç¼ºå¤±ï¼Œè«‹ç¢ºèªæ¨¡å‹æˆ–è³‡æ–™åº«ç‹€æ…‹å¾Œé‡è©¦ã€‚")
            return 2

        # æ¬„ä½æª¢æŸ¥
        missing_cols = check_missing_columns(engine, logger)

        # è‡ªå‹•è£œæ¬„ä½ï¼ˆåƒ…å®‰å…¨æ–°å¢ï¼‰
        if args.auto_fix and missing_cols:
            auto_fix_columns(engine, logger, missing_cols)
        elif missing_cols:
            logger.info("å¦‚éœ€è‡ªå‹•è£œä¸Šå¯å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼Œå¯ä½¿ç”¨ --auto-fix åƒæ•¸ã€‚")

        # ç´¢å¼•ç¢ºä¿
        ensure_indexes(engine, logger)

        # æœ€çµ‚çµ±è¨ˆ
        stats = get_database_stats(engine, logger)
        print_stats(stats, logger)

        logger.info("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼")
        if backup_path:
            logger.info(f"è‹¥éœ€å›å¾©ï¼Œå¯ä½¿ç”¨å‚™ä»½æª”ï¼š{backup_path}")
        return 0

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
