#!/usr/bin/env python3
"""
è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ï¼ˆä¸ä¾è³´ migrate.pyï¼‰

åŠŸèƒ½ï¼š
- ä»¥ app.models.database_models.Base ç‚ºå”¯ä¸€çœŸå¯¦ä¾†æºå»ºç«‹è³‡æ–™è¡¨
- æª¢æŸ¥ã€Œé‡è¦è¡¨ã€æ˜¯å¦å­˜åœ¨
- æƒæã€Œé—œéµæ¬„ä½ã€ç¼ºå¤±ï¼ˆé è¨­åƒ…å ±å‘Šï¼‰ï¼Œå¯é¸æ“‡å®‰å…¨è‡ªå‹•æ–°å¢ï¼ˆ--auto-fixï¼‰
- ç¢ºä¿å¸¸ç”¨ç´¢å¼•å­˜åœ¨
- SQLite è‡ªå‹•å‚™ä»½ï¼ˆå¯ç”¨ --no-backup é—œé–‰ï¼‰
- æä¾›çµ±è¨ˆè¼¸å‡ºï¼ˆ--stats-only åƒ…è¼¸å‡ºçµ±è¨ˆä¸è®Šæ›´è³‡æ–™åº«ï¼‰

ä½¿ç”¨ï¼š
  python database_init.py [--auto-fix] [--no-backup] [--stats-only] [--verbose | --quiet]

æ³¨æ„ï¼š
- åƒ…æ–°å¢æ¬„ä½ï¼Œä¸åšç ´å£æ€§è®Šæ›´ï¼ˆä¸æ”¹å‹ã€ä¸åˆªæ¬„ã€ä¸æ”¹éµ/ç´„æŸï¼‰
- åš´ç¦æ··ç”¨ä¸åŒ Baseï¼›æœ¬è…³æœ¬å›ºå®šæ¡ç”¨ app.models.database_models.Base
"""

from __future__ import annotations

import os
import sys
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

# ç¢ºä¿å°ˆæ¡ˆæ ¹ç›®éŒ„åœ¨åŒ¯å…¥è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from sqlalchemy import text
from sqlalchemy import inspect
from sqlalchemy.engine import Engine
from sqlalchemy.exc import SQLAlchemyError

from app.database import get_sync_engine  # ä½¿ç”¨åŒæ­¥å¼•æ“ç‚º database_init.py
from app.models.database_models import (
    Base,
    User, UserTeamPermission, ActiveSession, PasswordResetToken,  # èªè­‰ç³»çµ±ç›¸é—œè¡¨
    Team, TestRunConfig, TestRunItem, TestRunItemResultHistory,
    TCGRecord, LarkDepartment, LarkUser, SyncHistory,
)

# -----------------------------
# è¼”åŠ©è¼¸å‡ºï¼ˆç¹é«”ä¸­æ–‡ï¼‰
# -----------------------------
class Logger:
    def __init__(self, verbose: bool = False, quiet: bool = False):
        self.verbose = verbose
        self.quiet = quiet

    def info(self, msg: str):
        if not self.quiet:
            print(f"[INFO] {msg}")

    def debug(self, msg: str):
        if self.verbose and not self.quiet:
            print(f"[VERBOSE] {msg}")

    def warn(self, msg: str):
        print(f"[WARN] {msg}")

    def error(self, msg: str):
        print(f"[ERROR] {msg}")


# -----------------------------
# é€šç”¨å·¥å…·
# -----------------------------
IMPORTANT_TABLES: List[str] = [
    # èªè­‰ç³»çµ±ç›¸é—œè¡¨
    "users",
    "user_team_permissions", 
    "active_sessions",
    "password_reset_tokens",
    # æ¸¬è©¦ç³»çµ±ç›¸é—œè¡¨
    "teams",
    "test_run_configs",
    "test_run_items",
    "test_run_item_result_history",
    "tcg_records",
    "lark_departments",
    "lark_users",
    "sync_history",
]


def is_sqlite(engine: Engine) -> bool:
    return (engine.dialect.name or "").lower() == "sqlite"


def quote_ident(engine: Engine, name: str) -> str:
    return engine.dialect.identifier_preparer.quote(name)


# æ¬„ä½è¦æ ¼
class ColumnSpec:
    def __init__(self, name: str, type_sql: str, nullable: bool = True,
                 default: Optional[Any] = None, notes: Optional[str] = None):
        self.name = name
        self.type_sql = type_sql
        self.nullable = nullable
        self.default = default
        self.notes = notes

    def safe_to_add_on(self, engine: Engine) -> bool:
        # å®‰å…¨æ–°å¢è¦å‰‡ï¼š
        # - å¯ç‚º NULL çš„æ¬„ä½
        # - æˆ– NOT NULL ä½†æä¾› DEFAULT
        if self.nullable:
            return True
        return self.default is not None

    def default_sql_literal(self) -> Optional[str]:
        if self.default is None:
            return None
        if isinstance(self.default, str):
            return "'" + self.default.replace("'", "''") + "'"
        if self.default is True:
            return "1"
        if self.default is False:
            return "0"
        if self.default is None:
            return "NULL"
        return str(self.default)


# æ¬„ä½ç´„æŸä¿®æ”¹è¦æ ¼
class ColumnConstraintChange:
    def __init__(self, table: str, column: str, old_constraint: str, new_constraint: str, notes: str = ""):
        self.table = table
        self.column = column
        self.old_constraint = old_constraint  # ä¾‹å¦‚ "NOT NULL"
        self.new_constraint = new_constraint  # ä¾‹å¦‚ "NULL"
        self.notes = notes

    def needs_migration(self, engine: Engine) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œç´„æŸé·ç§»"""
        try:
            existing_cols = get_existing_columns(engine, self.table)
            col_info = existing_cols.get(self.column.lower())
            if not col_info:
                return False
            
            # æª¢æŸ¥ NOT NULL ç´„æŸ
            if self.old_constraint == "NOT NULL" and self.new_constraint == "NULL":
                return col_info.get("notnull", False)  # å¦‚æœç›®å‰æ˜¯ NOT NULLï¼Œéœ€è¦é·ç§»
            return False
        except Exception:
            return False

# æ¬„ä½ç´„æŸè®Šæ›´æ¸…å–®
COLUMN_CONSTRAINT_CHANGES: List[ColumnConstraintChange] = [
    ColumnConstraintChange(
        table="users",
        column="email",
        old_constraint="NOT NULL",
        new_constraint="NULL",
        notes="ç³»çµ±åˆå§‹åŒ–æ™‚å…è¨±ä¸æä¾› email"
    ),
]

# æ¬„ä½æª¢æŸ¥æ¸…å–®ï¼ˆåƒ…åˆ—å‡ºå¯èƒ½åœ¨æ—¢æœ‰ DB ç¼ºå°‘ã€ä¸”å¯ç”±æˆ‘å€‘è¼•é‡è£œä¸Šçš„æ¬„ä½ï¼‰
COLUMN_CHECKS: Dict[str, List[ColumnSpec]] = {
    # Users æ¬„ä½è£œå……
    "users": [
        ColumnSpec("lark_user_id", "TEXT", nullable=True, default=None),
    ],
    # TestRunItem çµæœæª”æ¡ˆè¿½è¹¤æ¬„ä½
    "test_run_items": [
        ColumnSpec("result_files_uploaded", "INTEGER", nullable=False, default=0),
        ColumnSpec("result_files_count", "INTEGER", nullable=False, default=0),
        ColumnSpec("upload_history_json", "TEXT", nullable=True, default=None),
        # èˆŠæ¬„ä½æª¢æŸ¥ï¼ˆå­˜åœ¨å³å¯ï¼Œä¸æœƒè‡ªå‹•å»ºç«‹ NOT NULL ç„¡é è¨­çš„æ¬„ä½ï¼‰
        ColumnSpec("assignee_json", "TEXT", nullable=True, default=None),
        ColumnSpec("tcg_json", "TEXT", nullable=True, default=None),
        ColumnSpec("bug_tickets_json", "TEXT", nullable=True, default=None),
    ],
    # TestRunConfig çš„ TP ç¥¨æ¬„ä½èˆ‡é€šçŸ¥æ¬„ä½
    "test_run_configs": [
        # TP ç¥¨ç›¸é—œ
        ColumnSpec("related_tp_tickets_json", "TEXT", nullable=True, default=None),
        ColumnSpec("tp_tickets_search", "TEXT", nullable=True, default=None),
        # é€šçŸ¥ç›¸é—œï¼ˆå°æ‡‰ ORMï¼šnotifications_enabled, notify_chat_ids_json, notify_chat_names_snapshot, notify_chats_searchï¼‰
        ColumnSpec("notifications_enabled", "INTEGER", nullable=False, default=0),  # Boolean -> INTEGER(0/1)
        ColumnSpec("notify_chat_ids_json", "TEXT", nullable=True, default=None),
        ColumnSpec("notify_chat_names_snapshot", "TEXT", nullable=True, default=None),
        ColumnSpec("notify_chats_search", "TEXT", nullable=True, default=None),
    ],
    # TestCaseLocal éœ€è¦æ–°å¢çš„é™„ä»¶æ¨™è¨˜æ¬„ä½
    "test_cases": [
        ColumnSpec("has_attachments", "INTEGER", nullable=False, default=0, notes="æ˜¯å¦æœ‰é™„ä»¶ï¼ˆ0/1ï¼‰"),
        ColumnSpec("attachment_count", "INTEGER", nullable=False, default=0, notes="é™„ä»¶æ•¸é‡"),
    ],
    # Lark Users é‡è¦ç´¢å¼•æ¬„ä½ï¼ˆè‹¥ç¼ºå°‘æ¬„ä½å‰‡åƒ…å ±å‘Šï¼Œä¸å¼·åˆ¶æ–°å¢ NOT NULLï¼‰
    "lark_users": [
        ColumnSpec("enterprise_email", "TEXT", nullable=True, default=None),
        ColumnSpec("primary_department_id", "TEXT", nullable=True, default=None),
    ],
}

# ç´¢å¼•è¦æ ¼
INDEX_SPECS: List[Dict[str, Any]] = [
    # Users
    {"name": "idx_users_lark_user_id", "table": "users", "columns": ["lark_user_id"], "unique": True},
    {"name": "idx_tri_configid_testcaseno", "table": "test_run_items", "columns": ["config_id", "test_case_number"]},
    {"name": "idx_tri_teamid_result", "table": "test_run_items", "columns": ["team_id", "test_result"]},
    {"name": "idx_tri_result_files_uploaded", "table": "test_run_items", "columns": ["result_files_uploaded"]},
    # test_run_configs ç›¸é—œæœå°‹æ¬„ä½ç´¢å¼•ï¼ˆè‹¥ ORM å·²å»ºç«‹ï¼Œé€™è£¡ä»¥ IF NOT EXISTS å½¢å¼è£œå¼·ï¼‰
    {"name": "idx_trc_tp_tickets_search", "table": "test_run_configs", "columns": ["tp_tickets_search"]},
    {"name": "idx_trc_notify_chats_search", "table": "test_run_configs", "columns": ["notify_chats_search"]},
    # Lark Users å¸¸ç”¨ç´¢å¼•
    {"name": "idx_lu_enterprise_email", "table": "lark_users", "columns": ["enterprise_email"]},
    {"name": "idx_lu_primary_department_id", "table": "lark_users", "columns": ["primary_department_id"]},
    # Sync History
    {"name": "idx_sh_teamid_starttime", "table": "sync_history", "columns": ["team_id", "start_time"]},
]


# -----------------------------
# æ ¸å¿ƒæ­¥é©Ÿå¯¦ä½œ
# -----------------------------

def backup_sqlite_if_needed(engine: Engine, logger: Logger) -> Optional[str]:
    if not is_sqlite(engine):
        logger.debug("é SQLiteï¼Œç•¥éå‚™ä»½ç¨‹åº")
        return None
    db_path = engine.url.database
    if not db_path or db_path == ":memory:":
        logger.debug("SQLite è¨˜æ†¶é«”è³‡æ–™åº«ï¼Œç•¥éå‚™ä»½")
        return None
    if not os.path.exists(db_path):
        logger.debug(f"è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨ï¼ˆå°‡æ–¼ create_all æ™‚å»ºç«‹ï¼‰ï¼š{db_path}")
        return None
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"backup_init_{ts}.db"
    try:
        shutil.copy2(db_path, backup_path)
        logger.info(f"å·²å»ºç«‹ SQLite å‚™ä»½ï¼š{backup_path}")
        return backup_path
    except Exception as e:
        logger.warn(f"å»ºç«‹å‚™ä»½å¤±æ•—ï¼ˆä¸ä¸­æ–·ï¼‰ï¼š{e}")
        return None


def _migrate_legacy_auth_tables(engine: Engine, logger: Logger):
    """é·ç§»èˆŠçš„èªè­‰ç›¸é—œè¡¨æ ¼åˆ°æ–°çš„çµæ§‹"""
    with engine.begin() as conn:
        # æª¢æŸ¥èˆŠ users è¡¨æ ¼çš„çµæ§‹
        result = conn.execute(text("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name = 'users'
        """))
        has_old_users = result.fetchone() is not None
        
        if has_old_users:
            # æª¢æŸ¥è¡¨æ ¼çµæ§‹æ˜¯å¦ç‚ºèˆŠç‰ˆæœ¬
            result = conn.execute(text("PRAGMA table_info(users)"))
            columns = [row[1] for row in result.fetchall()]
            
            # å¦‚æœæ˜¯èˆŠç‰ˆæœ¬çš„ users è¡¨ï¼ˆæœ‰ lark_id ä½†æ²’æœ‰ username/emailï¼‰
            if 'lark_id' in columns and 'username' not in columns:
                logger.info("æª¢æ¸¬åˆ°èˆŠç‰ˆ users è¡¨æ ¼ï¼Œé€²è¡Œçµæ§‹é·ç§»...")
                
                # å‚™ä»½èˆŠè¡¨æ ¼
                conn.execute(text("CREATE TABLE users_legacy_backup AS SELECT * FROM users"))
                logger.info("å·²å‚™ä»½èˆŠ users è¡¨æ ¼è‡³ users_legacy_backup")
                
                # åˆªé™¤èˆŠè¡¨æ ¼
                conn.execute(text("DROP TABLE users"))
                
                # ç”± SQLAlchemy é‡æ–°å‰µåººæ–°è¡¨æ ¼ï¼ˆåœ¨ Base.metadata.create_all ä¸­è™•ç†ï¼‰
                logger.info("å·²åˆªé™¤èˆŠ users è¡¨ï¼Œå°‡ç”± SQLAlchemy é‡æ–°å‰µåºº")
            else:
                logger.info("ç¾æœ‰ users è¡¨æ ¼çµæ§‹å·²ç‚ºæ–°ç‰ˆï¼Œç„¡éœ€é·ç§»")
                
        # è™•ç† roles è¡¨æ ¼ï¼ˆæ–°ç³»çµ±ä¸éœ€è¦ç¨ç«‹çš„ roles è¡¨ï¼‰
        result = conn.execute(text("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name = 'roles'
        """))
        has_old_roles = result.fetchone() is not None
        
        if has_old_roles:
            logger.info("æª¢æ¸¬åˆ°èˆŠçš„ roles è¡¨æ ¼ï¼Œå‚™ä»½å¾Œåˆªé™¤...")
            conn.execute(text("CREATE TABLE roles_legacy_backup AS SELECT * FROM roles"))
            conn.execute(text("DROP TABLE roles"))
            logger.info("èˆŠ roles è¡¨æ ¼å·²å‚™ä»½è‡³ roles_legacy_backup ä¸¦åˆªé™¤")


def ensure_test_run_item_history_fk(engine: Engine, logger: Logger) -> None:
    """ä¿®å¾© test_run_item_result_history è¡¨æ ¼ä»å¼•ç”¨èˆŠå‚™ä»½è¡¨çš„å¤–éµã€‚"""
    if not is_sqlite(engine):
        return

    with engine.begin() as connection:
        row = connection.execute(
            text(
                "SELECT sql FROM sqlite_master "
                "WHERE type='table' AND name='test_run_item_result_history'"
            )
        ).fetchone()
        if not row or not row[0] or "test_run_items_backup_snapshot" not in row[0]:
            return

        logger.info("åµæ¸¬åˆ° test_run_item_result_history å¤–éµå¼•ç”¨å‚™ä»½è¡¨ï¼Œé–‹å§‹ä¿®å¾©")
        legacy_name = "test_run_item_result_history_legacy_fk"
        connection.execute(text("PRAGMA foreign_keys=OFF"))
        try:
            connection.execute(text(f"ALTER TABLE test_run_item_result_history RENAME TO {legacy_name}"))

            indexes = connection.execute(
                text(
                    "SELECT name FROM sqlite_master "
                    "WHERE type='index' AND tbl_name=:tbl AND name NOT LIKE 'sqlite_autoindex%'"
                ),
                {"tbl": legacy_name},
            ).fetchall()
            for (index_name,) in indexes:
                if index_name:
                    connection.execute(text(f'DROP INDEX IF EXISTS "{index_name}"'))

            TestRunItemResultHistory.__table__.create(bind=connection)

            available_cols = connection.execute(
                text(f"PRAGMA table_info('{legacy_name}')")
            ).fetchall()
            available = {row[1] for row in available_cols}
            target_columns = [
                col.name
                for col in TestRunItemResultHistory.__table__.c
                if col.name in available
            ]
            if target_columns:
                columns_csv = ", ".join(f'"{col}"' for col in target_columns)
                connection.execute(
                    text(
                        f'INSERT INTO test_run_item_result_history ({columns_csv}) '
                        f'SELECT {columns_csv} FROM {legacy_name}'
                    )
                )

            connection.execute(text(f"DROP TABLE {legacy_name}"))

            try:
                connection.execute(
                    text(
                        "UPDATE sqlite_sequence SET seq = "
                        "COALESCE((SELECT MAX(id) FROM test_run_item_result_history), 0) "
                        "WHERE name='test_run_item_result_history'"
                    )
                )
            except Exception:
                pass
            logger.info("test_run_item_result_history å¤–éµä¿®å¾©å®Œæˆ")
        finally:
            connection.execute(text("PRAGMA foreign_keys=ON"))


def create_all_tables(engine: Engine, logger: Logger):
    logger.info("å»ºç«‹/ç¢ºä¿æ‰€æœ‰è³‡æ–™è¡¨ï¼ˆä¾æ“š ORM æ¨¡å‹ï¼‰...")
    try:
        # åŸ·è¡Œé·ç§»
        _migrate_legacy_auth_tables(engine, logger)
        
        # å‰µåººæ‰€æœ‰è¡¨æ ¼
        Base.metadata.create_all(bind=engine)
        
        # åŸ·è¡Œ FK ä¿®æ­£
        ensure_test_run_item_history_fk(engine, logger)
        
        logger.info("è³‡æ–™è¡¨ç¢ºèªå®Œæˆ")
    except SQLAlchemyError as e:
        raise RuntimeError(f"å»ºç«‹è³‡æ–™è¡¨å¤±æ•—ï¼š{e}")


def verify_required_tables(engine: Engine, logger: Logger) -> Tuple[bool, List[str]]:
    inspector = inspect(engine)
    existing = {t.lower() for t in inspector.get_table_names()}
    missing = [t for t in IMPORTANT_TABLES if t.lower() not in existing]
    if missing:
        logger.error(f"ç¼ºå°‘é‡è¦è¡¨ï¼š{missing}")
        return False, missing
    logger.debug("æ‰€æœ‰é‡è¦è¡¨çš†å­˜åœ¨")
    return True, []


def get_existing_columns(engine: Engine, table_name: str) -> Dict[str, Dict[str, Any]]:
    # ä»¥å°å¯« key å›å‚³
    result: Dict[str, Dict[str, Any]] = {}
    if is_sqlite(engine):
        with engine.connect() as conn:
            rows = conn.exec_driver_sql(f"PRAGMA table_info({table_name})").fetchall()
            # PRAGMA columns: cid, name, type, notnull, dflt_value, pk
            for _, name, typ, notnull, dflt, _ in rows:
                result[(name or "").lower()] = {
                    "name": name,
                    "type": typ,
                    "notnull": bool(notnull),
                    "default": dflt,
                }
    else:
        inspector = inspect(engine)
        cols = inspector.get_columns(table_name)
        for col in cols:
            result[(col.get("name") or "").lower()] = col
    return result


def check_missing_columns(engine: Engine, logger: Logger) -> Dict[str, List[ColumnSpec]]:
    missing: Dict[str, List[ColumnSpec]] = {}
    for table, specs in COLUMN_CHECKS.items():
        try:
            existing = get_existing_columns(engine, table)
        except Exception:
            # è¡¨ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œäº¤ç”± verify_required_tables å…ˆè¡Œè™•ç†
            continue
        for spec in specs:
            if spec.name.lower() not in existing:
                missing.setdefault(table, []).append(spec)
    if missing:
        logger.warn("åµæ¸¬åˆ°ç¼ºå¤±æ¬„ä½ï¼ˆé è¨­åƒ…å ±å‘Šï¼Œä¸è‡ªå‹•ä¿®å¾©ï¼‰ï¼š")
        for table, specs in missing.items():
            for spec in specs:
                fixable = "å¯å®‰å…¨æ–°å¢" if spec.safe_to_add_on(engine) else "éœ€äººå·¥è™•ç†"
                logger.warn(f"  - {table}.{spec.name} ({spec.type_sql}) -> {fixable}{'ï½œ' + spec.notes if spec.notes else ''}")
    else:
        logger.info("æœªç™¼ç¾éœ€è£œå……çš„æ¬„ä½")
    return missing


def check_constraint_changes(engine: Engine, logger: Logger) -> List[ColumnConstraintChange]:
    """æª¢æŸ¥éœ€è¦é€²è¡Œç´„æŸè®Šæ›´çš„æ¬„ä½"""
    needed_changes = []
    for change in COLUMN_CONSTRAINT_CHANGES:
        if change.needs_migration(engine):
            needed_changes.append(change)
    
    if needed_changes:
        logger.warn("åµæ¸¬åˆ°éœ€è¦ç´„æŸè®Šæ›´çš„æ¬„ä½ï¼ˆé è¨­åƒ…å ±å‘Šï¼Œä¸è‡ªå‹•ä¿®å¾©ï¼‰ï¼š")
        for change in needed_changes:
            logger.warn(f"  - {change.table}.{change.column}: {change.old_constraint} -> {change.new_constraint} {'ï½œ' + change.notes if change.notes else ''}")
    else:
        logger.info("æœªç™¼ç¾éœ€è¦ç´„æŸè®Šæ›´çš„æ¬„ä½")
    
    return needed_changes

def auto_fix_constraint_changes(engine: Engine, logger: Logger, constraint_changes: List[ColumnConstraintChange]):
    """è‡ªå‹•ä¿®å¾©æ¬„ä½ç´„æŸè®Šæ›´ï¼ˆåƒ…é™ SQLiteï¼‰"""
    if not constraint_changes:
        logger.info("ç„¡ç´„æŸè®Šæ›´éœ€è¦ä¿®å¾©")
        return
    
    if not is_sqlite(engine):
        logger.warn("ç´„æŸè®Šæ›´ä¿®å¾©ç›®å‰åƒ…æ”¯æ´ SQLiteè³‡æ–™åº«")
        return
    
    logger.info("é–‹å§‹è‡ªå‹•ä¿®å¾©æ¬„ä½ç´„æŸè®Šæ›´...")
    
    for change in constraint_changes:
        if change.table == "users" and change.column == "email" and change.old_constraint == "NOT NULL":
            try:
                _migrate_users_email_to_nullable(engine, logger)
                logger.info(f"å·²ä¿®å¾©ç´„æŸï¼š{change.table}.{change.column}")
            except Exception as e:
                logger.error(f"ä¿®å¾©ç´„æŸå¤±æ•—ï¼š{change.table}.{change.column} -> {e}")
        else:
            logger.warn(f"è·³éä¸æ”¯æ´çš„ç´„æŸè®Šæ›´ï¼š{change.table}.{change.column}")

def _migrate_users_email_to_nullable(engine: Engine, logger: Logger):
    """å°‡ users.email æ¬„ä½å¾ NOT NULL æ”¹ç‚º nullable"""
    logger.info("é–‹å§‹é·ç§» users.email æ¬„ä½ç´„æŸ...")
    
    with engine.begin() as conn:
        # å‚™ä»½åŸå§‹è¡¨çµæ§‹ç‚ºä¸´æ™‚è¡¨
        conn.exec_driver_sql("""
            CREATE TABLE users_backup AS 
            SELECT * FROM users
        """)
        
        # å»ºç«‹æ–°çš„ users è¡¨çµæ§‹ï¼ˆemail å¯ç‚º NULLï¼‰
        conn.exec_driver_sql("DROP TABLE users")
        conn.exec_driver_sql("""
            CREATE TABLE users (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                username VARCHAR(50) UNIQUE NOT NULL,
                email VARCHAR(255) UNIQUE,
                hashed_password VARCHAR(255) NOT NULL,
                full_name VARCHAR(255),
                role VARCHAR(50) NOT NULL DEFAULT 'user',
                is_active BOOLEAN NOT NULL DEFAULT 1,
                is_verified BOOLEAN NOT NULL DEFAULT 0,
                created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
                last_login_at DATETIME
            )
        """)
        
        # å¾©åŸè³‡æ–™ï¼ˆç©ºå­—ä¸² email è½‰ç‚º NULLï¼‰
        conn.exec_driver_sql("""
            INSERT INTO users (
                id, username, email, hashed_password, full_name, 
                role, is_active, is_verified, created_at, updated_at, last_login_at
            )
            SELECT 
                id, username, 
                CASE WHEN email = '' OR email IS NULL THEN NULL ELSE email END,
                hashed_password, full_name, 
                role, is_active, is_verified, created_at, updated_at, last_login_at
            FROM users_backup
        """)
        
        # é‡å»ºç´¢å¼•
        conn.exec_driver_sql("CREATE INDEX ix_users_username ON users (username)")
        conn.exec_driver_sql("CREATE INDEX ix_users_email ON users (email)")
        conn.exec_driver_sql("CREATE INDEX ix_users_role_active ON users (role, is_active)")
        conn.exec_driver_sql("CREATE INDEX ix_users_email_active ON users (email, is_active)")
        conn.exec_driver_sql("CREATE INDEX ix_users_is_active ON users (is_active)")
        
        # æ¸…ç†å‚™ä»½è¡¨
        conn.exec_driver_sql("DROP TABLE users_backup")
    
    logger.info("users.email æ¬„ä½ç´„æŸé·ç§»å®Œæˆ")

def auto_fix_columns(engine: Engine, logger: Logger, missing: Dict[str, List[ColumnSpec]]):
    if not missing:
        logger.info("ç„¡æ¬„ä½éœ€è¦è‡ªå‹•ä¿®å¾©")
        return
    logger.info("é–‹å§‹è‡ªå‹•æ–°å¢å®‰å…¨æ¬„ä½ï¼ˆåƒ…é™å¯å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼‰...")
    for table, specs in missing.items():
        for spec in specs:
            if not spec.safe_to_add_on(engine):
                logger.warn(f"è·³éä¸å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼š{table}.{spec.name}ï¼ˆNOT NULL ä¸”ç„¡ DEFAULT æˆ–éœ€äººå·¥é·ç§»ï¼‰")
                continue
            parts = [spec.type_sql]
            default_sql = spec.default_sql_literal()
            if default_sql is not None:
                parts.append(f"DEFAULT {default_sql}")
            if not spec.nullable:
                parts.append("NOT NULL")
            col_ddl = " ".join(parts)
            sql = f"ALTER TABLE {quote_ident(engine, table)} ADD COLUMN {quote_ident(engine, spec.name)} {col_ddl}"
            try:
                with engine.begin() as conn:
                    conn.exec_driver_sql(sql)
                logger.info(f"å·²æ–°å¢æ¬„ä½ï¼š{table}.{spec.name}")
            except Exception as e:
                logger.warn(f"æ–°å¢æ¬„ä½å¤±æ•—ï¼š{table}.{spec.name} -> {e}")


def ensure_indexes(engine: Engine, logger: Logger):
    logger.info("ç¢ºä¿å¸¸ç”¨ç´¢å¼•å­˜åœ¨...")
    dialect = (engine.dialect.name or "").lower()
    supports_if_not_exists = dialect in {"sqlite", "postgresql"}
    inspector = inspect(engine)

    for idx in INDEX_SPECS:
        name = idx["name"]
        table = idx["table"]
        columns = idx["columns"]
        try:
            existing = {i.get("name") for i in inspector.get_indexes(table)}
        except Exception:
            existing = set()
        if name in existing:
            logger.debug(f"ç´¢å¼•å·²å­˜åœ¨ï¼š{name}")
            continue
        cols_sql = ", ".join(quote_ident(engine, c) for c in columns)
        is_unique = bool(idx.get("unique", False))
        if supports_if_not_exists:
            sql = f"CREATE {'UNIQUE ' if is_unique else ''}INDEX IF NOT EXISTS {quote_ident(engine, name)} ON {quote_ident(engine, table)} ({cols_sql})"
        else:
            sql = f"CREATE {'UNIQUE ' if is_unique else ''}INDEX {quote_ident(engine, name)} ON {quote_ident(engine, table)} ({cols_sql})"
        try:
            with engine.begin() as conn:
                conn.exec_driver_sql(sql)
            logger.info(f"å·²å»ºç«‹ç´¢å¼•ï¼š{name}")
        except Exception as e:
            # å¯èƒ½ç«¶æ…‹æˆ–å·²å­˜åœ¨ç­‰æƒ…æ³
            logger.warn(f"å»ºç«‹ç´¢å¼•è­¦å‘Šï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰ï¼š{name} -> {e}")


def get_database_stats(engine: Engine, logger: Logger) -> Dict[str, Any]:
    stats: Dict[str, Any] = {"tables": {}, "total_tables": 0, "engine_url": str(engine.url), "errors": []}
    try:
        if is_sqlite(engine):
            with engine.connect() as conn:
                rows = conn.exec_driver_sql(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
                ).fetchall()
                table_names = [r[0] for r in rows]
                for t in table_names:
                    try:
                        cnt = conn.exec_driver_sql(f"SELECT COUNT(*) FROM {quote_ident(engine, t)}").scalar()
                        cols = conn.exec_driver_sql(f"PRAGMA table_info({t})").fetchall()
                        stats["tables"][t] = {
                            "rows": int(cnt or 0),
                            "columns": len(cols),
                        }
                    except Exception as e:
                        stats["tables"][t] = {"error": str(e)}
        else:
            inspector = inspect(engine)
            table_names = inspector.get_table_names()
            with engine.connect() as conn:
                for t in table_names:
                    try:
                        cnt = conn.execute(text(f"SELECT COUNT(*) FROM {quote_ident(engine, t)}")).scalar()
                        cols = inspector.get_columns(t)
                        stats["tables"][t] = {
                            "rows": int(cnt or 0),
                            "columns": len(cols),
                        }
                    except Exception as e:
                        stats["tables"][t] = {"error": str(e)}
        stats["total_tables"] = len(stats["tables"])
    except Exception as e:
        stats["errors"].append(str(e))
    return stats


def print_stats(stats: Dict[str, Any], logger: Logger):
    print("=" * 60)
    print("ğŸ“Š è³‡æ–™åº«çµ±è¨ˆæ‘˜è¦")
    print("=" * 60)
    print(f"ç¸½è¡¨æ ¼æ•¸ï¼š{stats.get('total_tables')}")
    tables = stats.get("tables", {})
    for t, d in sorted(tables.items()):
        if "error" in d:
            print(f"  âŒ {t}: {d['error']}")
        else:
            print(f"  âœ… {t}: {d['rows']} ç­†è¨˜éŒ„, {d['columns']} æ¬„ä½")
    print()
    print("é‡è¦è¡¨æ ¼ç‹€æ…‹ï¼š")
    for t in IMPORTANT_TABLES:
        d = tables.get(t)
        if d is None:
            print(f"  âš ï¸  {t}: è¡¨æ ¼ä¸å­˜åœ¨")
        elif "error" in d:
            print(f"  âŒ {t}: {d['error']}")
        else:
            print(f"  âœ… {t}: {d['rows']} ç­†è¨˜éŒ„, {d['columns']} æ¬„ä½")
    print()
    print(f"ğŸ“‚ è³‡æ–™åº«ä½ç½®ï¼š{stats.get('engine_url')}")


# -----------------------------
# åƒæ•¸èˆ‡ä¸»æµç¨‹
# -----------------------------

def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="è³‡æ–™åº«åˆå§‹åŒ–è…³æœ¬ï¼ˆä¸ä¾è³´ migrate.pyï¼‰")
    p.add_argument("--auto-fix", action="store_true", help="è‡ªå‹•æ–°å¢å¯å®‰å…¨æ–°å¢çš„ç¼ºå¤±æ¬„ä½")
    p.add_argument("--no-backup", action="store_true", help="ï¼ˆSQLiteï¼‰è·³éåˆå§‹åŒ–å‰çš„è³‡æ–™åº«å‚™ä»½")
    p.add_argument("--stats-only", action="store_true", help="åƒ…è¼¸å‡ºçµ±è¨ˆèˆ‡ç‹€æ…‹ï¼Œä¸åšä»»ä½•è®Šæ›´")
    g = p.add_mutually_exclusive_group()
    g.add_argument("--verbose", action="store_true", help="è¼¸å‡ºæ›´å¤šè©³ç´°è³‡è¨Š")
    g.add_argument("--quiet", action="store_true", help="åƒ…è¼¸å‡ºå¿…è¦è³‡è¨Šèˆ‡éŒ¯èª¤")
    return p.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv)
    logger = Logger(verbose=args.verbose, quiet=args.quiet)

    print("=" * 60)
    print("ğŸ—ƒï¸  è³‡æ–™åº«åˆå§‹åŒ–ç³»çµ±ï¼ˆä¸ä¾è³´ migrate.pyï¼‰")
    print("=" * 60)

    try:
        # ç²å–åŒæ­¥å¼•æ“
        engine = get_sync_engine()
        db_url = str(engine.url)
        db_kind = engine.dialect.name
        logger.info(f"åµæ¸¬åˆ°è³‡æ–™åº«ï¼š{db_kind} | URL={db_url}")

        if args.stats_only:
            stats = get_database_stats(engine, logger)
            print_stats(stats, logger)
            return 0

        # å‚™ä»½ï¼ˆSQLiteï¼‰
        backup_path = None
        if is_sqlite(engine) and not args.no_backup:
            backup_path = backup_sqlite_if_needed(engine, logger)

        # å»ºè¡¨
        create_all_tables(engine, logger)

        # é©—è­‰é‡è¦è¡¨
        ok, missing = verify_required_tables(engine, logger)
        if not ok:
            logger.error("é‡è¦è¡¨ç¼ºå¤±ï¼Œè«‹ç¢ºèªæ¨¡å‹æˆ–è³‡æ–™åº«ç‹€æ…‹å¾Œé‡è©¦ã€‚")
            return 2

        # æ¬„ä½æª¢æŸ¥
        missing_cols = check_missing_columns(engine, logger)
        
        # ç´„æŸè®Šæ›´æª¢æŸ¥
        constraint_changes = check_constraint_changes(engine, logger)

        # è‡ªå‹•è£œæ¬„ä½ï¼ˆåƒ…å®‰å…¨æ–°å¢ï¼‰
        if args.auto_fix and missing_cols:
            auto_fix_columns(engine, logger, missing_cols)
        elif missing_cols:
            logger.info("å¦‚éœ€è‡ªå‹•è£œä¸Šå¯å®‰å…¨æ–°å¢çš„æ¬„ä½ï¼Œå¯ä½¿ç”¨ --auto-fix åƒæ•¸ã€‚")
        
        # è‡ªå‹•ä¿®å¾©ç´„æŸè®Šæ›´
        if args.auto_fix and constraint_changes:
            auto_fix_constraint_changes(engine, logger, constraint_changes)
        elif constraint_changes:
            logger.info("å¦‚éœ€è‡ªå‹•ä¿®å¾©ç´„æŸè®Šæ›´ï¼Œå¯ä½¿ç”¨ --auto-fix åƒæ•¸ã€‚")

        # ç´¢å¼•ç¢ºä¿
        ensure_indexes(engine, logger)

        # æœ€çµ‚çµ±è¨ˆ
        stats = get_database_stats(engine, logger)
        print_stats(stats, logger)

        logger.info("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆï¼")
        if backup_path:
            logger.info(f"è‹¥éœ€å›å¾©ï¼Œå¯ä½¿ç”¨å‚™ä»½æª”ï¼š{backup_path}")
        return 0

    except Exception as e:
        logger.error(f"åˆå§‹åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
